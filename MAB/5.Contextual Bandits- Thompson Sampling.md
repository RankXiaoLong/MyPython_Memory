<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=6 ><b>5.Contextual Bandits: Thompson Sampling</b></font></center> 

---



- Bernoulli Bandit: 一道菜概率$p = \theta$做的好吃，以概率$p=(1-\theta)$做的不好吃
- Bayesian (贝叶斯) 学派认为应该用概率分布来描述$\theta$的不确定性
  - $p (\theta|reward) \propto p(reward|\theta)p(\theta) = Bernoulli(\theta)Beta(\alpha,\beta)$

### 1.从Bernoulli Bandit到Contextual Bandit

我们先对比一下Bernoulli Bandit和Contextual Bandit的不同：

- 在Bernoulli Bandit中，我们假设reward是服从伯努利分布的$reward \sim Bernoulli(\theta)$
- 在Contextual Bandit中，我们假设reward和特征向量存在一个线性关系$reward = x^T \theta$，是确定性的，无法直接定义出一个概率分布来描述$\theta$的不确定性

如果我们可以定义$p(\theta|reward)$，那么我们就可以使用Thompson Sampling来解决Contextual Bandit：

- 步骤1:用$p(\theta|reward)$为每道菜刻画$\theta$的不确定性，得到$\{p(\theta_1|reward_1),...,p(\theta_N|reward_N)\}$
- 步骤2:对每道菜$p(\theta_i|reward_i)$随机抽取一个样本$\theta_i$，得到$\{\theta_1,...,\theta_N\}$
- 步骤3:推荐$x_i^T \theta_i$最大的那道菜，观测$reward_i$
- 步骤4:更新$\theta_i$的分布: $p(\theta|reward) \propto p(reward|\theta)p(\theta)$

万事具备，只欠东风－下面我们介绍如何定义$p(\theta|reward)$

### 2.如何定义$p(\theta|reward)$

首先依据贝叶斯公式，我们有
$$
p(\theta|reward) \propto p(reward|\theta)p(\theta)
$$
定义$p(\theta|reward)$转化为了定义$p(reward|\theta)$和$p(\theta)$:

- 定义$p(reward|\theta)$: 我们估计reward为$x^T \theta$，机器学习中常用的trick是假设真实的reward服从以$x^T \theta$为中心、 $\sigma$为标准差的正态分布，即$p(reward|\theta) = N(x^T \theta,\sigma)$
- 定义$p(\theta)$: 为了数学方便，$p(\theta)$一般会选择与$p(reward|\theta)$的共轭。既然$p(reward|\theta$)是正态分布，那么$p(\theta)$也应该是正态分布（正态分布的共轭还是正态分布），即 $p(\theta) \sim N(0,I)$

最后可以得出$p(\theta|reward) = N(\mu' ,\sigma') \propto p(reward|\theta)p(\theta) = N(x^T \theta,\sigma)N(0,I)$，其中

- $\mu' = (X^TX+I)^{-1} X^T rewards$
- $\sigma' = v^2 (X^TX+I)^{-1}$


#### 参考文献

[1] [Thompson Sampling for Contextual Bandits with Linear Payoffs](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1209.3352.pdf)

[2] [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1003.0146.pdf)

[3] [原文](https://zhuanlan.zhihu.com/p/32429623)

