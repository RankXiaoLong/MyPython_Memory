<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=8 ><b>4.Contextual Bandits: LinUCB</b></font></center> 

---





**拓展1：给定时间（早中晚餐）、客户的年龄、性别，推荐哪个用户才喜欢？456所示**



假设我们开了一家叫Surprise Me的饭馆

- 客人来了不用点餐，由算法从N道菜中选择一道菜推荐给客人
- 算法的目标是让满意的客人越多越好。

解决方法 - **U**pper **C**onfidence **B**ound (UCB) 算法



- 基于目前的数据，估计出每道菜被接受的概率$\tilde{p}$以及浮动范围$\Delta$
- 我们乐观的认为下一个应该推荐的菜应该是$\tilde{p} + \Delta$最大的那个
- 影响浮动范围$\Delta$的因素
  - 对于被选中的菜，多获得一次反馈会使$\Delta$变小，最终会小于其他没有被选中的菜
  - 对于没被选中的菜，$\Delta$对于没被选中的菜，![[公式]](https://www.zhihu.com/equation?tex=%5CDelta) 会随着轮数的增大而增大，最终会大于其他被选中的菜$\Delta$



### 1.Multi-Armed **B**andit (MAB) 存在的问题

MAB将每道菜看成是独立的个体，缺乏用附加信息刻画决策过程的机制（菜的类型、客户的年龄性别、早中晚饭）：

- 忽略了菜的属性

- - 菜是可以分成素菜、荤菜

- 忽略了用户之间的口味差别

- - 用户可以用年龄、性别来刻画，不同的用户的口味是不一样的

- 忽略了就餐时间

- - 早、中、晚餐的偏好常常是不同的

- 

### 2.**C**ontextual **B**andits (CB) ：用附加信息刻画决策过程

在Contextual Bandits的世界里，每次做决定都是依据了菜、客户、时间的属性：

- 步骤 1：观测到特征向量$x = (荤菜，素菜，男，女，早饭，午饭，晚饭)$
- 步骤 2：预测客人是否会接受这道菜：$\tilde{p} = x^T \theta$，其中$\theta$是需要学习的系数（如何学习会在后面介绍）
- 步骤 3：按照步骤1、2对所有的菜都进行预测得到 $(\tilde{p}_1,...,\tilde{p}_N)$，进行推荐（这里同样存在Exploration vs Exploitation问题，会在后面介绍）

可以看出Contextual Bandits是更加贴近实际环境的：

- 定义荤素菜、客人的年龄性别、就餐时间这些特征本质上是在刻画决策的依据
- 使用线性回归模型来预测reward，本质上是在模仿人类决策过程，当然也有相关论文使用非线性模型
- 菜的品种千千万万，但是不同的菜的特征向量可能是重叠的（例如都是荤菜），所以CB不需要对每道菜都收集反馈，而MAB需要对每道菜都收集反馈。

对于Contextual Bandits，同样存在Exploration vs Exploitation的问题：虽然可以通过线性回归得到一道菜接受的概率$\tilde{p}$，但这个概率总是有偏差的，下面我们介绍如何将MAB中的UCB算法拓展到Contextual Bandits中。



### 3.Contextual Bandits(CB): 如何学习$\theta$

在Contextual Bandits当中，$\tilde{p} = x^T \theta$，其中$\theta$是需要学习的参数

下面我们介绍最经常使用的线性回归－Ridge Regression来求解$\theta$：

- 输入：多次试验结果$\{(x_1,reward_1),...,(x_N,reward_N)\}$

- 优化目标如下
  $$
  Loss(\theta) = \| X\theta - rewards\|^2 + \| I\theta\|^2
  $$
  其中

  - $X=[x_1,...,x_K]$是K次实验观测的特征向量组成的矩阵，每一行代表一个特征向量
  - $rewards = [reward_1,...,reward_K]$是K次试验的结果， reward=1表示客人接受了这道菜，reward=0表示客人离开
  - $\|IX\|^2$，也就是常用的L2 normalization，可以有效防止主要是为了方式过拟合，其中I为对角线矩阵

求解$\theta$可以对$Loss(\theta)$求导:
$$
\frac{Loss(\theta)}{\theta} = 2X^T (X\theta - rewards) + 2I^T I \theta = 0
$$
得
$$
\theta = (X^TX+I^TI)^{-1}X^T rewards
$$
得到了$\theta$便可以方便的计算$\tilde{p} = x^T \theta$，但是$\tilde{p}$和真实的概率$p$总会存在一个差值$\Delta$，下面一节着重介绍如何求解这个差值。



### 4.LinUCB：从MAB到Contextual Bandits

**U**pper **C**onfidence **B**ound (UCB) 的思想也是适用在Contextual Bandits上，UCB的基本思想是：

- 通过线性回归得到$\tilde{p}$和真实的概率$p$总会存在一个差值$\Delta$，即$\tilde{p} - \Delta \leq p \leq \tilde{p} + \Delta$
- **我们乐观地认为每道菜能够获得的回报是**$\tilde{p}+\Delta$，总是按$\tilde{p}+\Delta$对所有的菜进行排序

在MAB中，$\tilde{p} = \sum_i reward_i/n$，其中$\Delta$通过Chernoff-Hoeffding Bound得到的，reward是在[0, 1]之间独立同分布的。

在Contextual Bandits当中有$\tilde{p} = x^T \theta$,其中$\theta$是根据多次试验$\{(x_1,reward_1),...,(x_N,reward_N)\}$求解Ridge Regression得到的，缺少一个“类似Chernoff-Hoeffding Bound的定理”来量化 $\Delta$，幸运的是，[已经有人发现了这个定理](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1205.2606)：
$$
P\{|x^T \theta -p| \leq (1+ \sqrt{\ln(2/\delta)/2}) \sqrt{x^T (X^TX+I^TI)^{-1}x}  \} \leq 1-\delta
$$
因此$\Delta =  (1+ \sqrt{\ln(2/\delta)/2}) \sqrt{x^T (X^TX+I^TI)^{-1}x}$，其中$\delta$是超参数。

示意代码总结如下

```python
def LinUCB(items, N):
    upper_bound_probs = [items[i].mul(theta) + compute_delta(X) for i in range(N)]
    item = np.argmax(upper_bound_probs)
    reward = np.random.binomial(n=1, p=true_rewards[item])
    return item, reward

for t in range(1, T): # T个客人依次进入餐馆
   items = [extract_feature(i, t) for i in range(N)]       
   item, reward = LinUCB(items, N)
   total_reward += reward # 一共有多少客人接受了推荐
```

