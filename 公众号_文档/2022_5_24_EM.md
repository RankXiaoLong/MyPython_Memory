<center> <font size=15 ><b>8.EM算法</b></font></center> 





## 1. 极大似然到EM



### 1.1 极大似然

$x_1,x_2,...,x_n$来自总体$N(\mu ,\sigma^2)$，有未知参数$\theta = [\mu,\sigma]^T$，有
$$ L(\theta)= \prod_{i=1}^n  p(x_i|\theta) , \theta \in \Theta \\
H(\theta) = \ln L(\theta = \sum_{i=1}^n \ln p(x_i|\theta) \\
\hat{\theta} = \arg\max L(\theta) $$


**求解极大似然的一般步骤**

（1）写出似然函数；

（2）对似然函数取对数，并整理；

（3）求导数，令导数为 0，得到似然方程；

（4）解似然方程，得到的参数。



### 1.2 EM算法



$\{x_n\},\{y_m\}$分服从$N(\mu_1,\sigma^2_1),N(\mu_2,\sigma^2_2)$，来估计总体$\{x_n,y_m\}$的分布。抽取得到的每个样本都不知道是从哪个分布来的。

(**注意：EM算法和极大似然估计的前提是一样的，都要假设数据总体的分布，如果不知道数据分布，是无法使用EM算法的**)





#### 1.21 EM算法

此时有两个问题需要估计：

- 样本从哪个总体得到
- 样本对应的正态分布的参数是什么



**EM的意思是“Expectation Maximization”，具体方法为：**

- 先假定$x_i,y_j$的分布参数
- 然后计算出每个样本更可能属于第一个还是第二个正态分布中的，这个是属于Expectation 一步；
- 我们已经大概地按上面的方法所有样本，之后可以根据极大似然估计分别对$x_i,y_j$分布参数进行估计这步称为 Maximization；
- 然后，当我们更新这两个分布的时候，每个样本属于分布的概率变化，那么我们就再需要调整E步；
- ……如此往复，直到参数基本不再发生变化或满足结束条件为止。



#### 1.2.2 总结

上述样本属于的分布称为**隐含参数**，分布的参数称为**模型参数**

EM 算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM 算法的 E 步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM算法的 E 步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。





## 2.EM算法推导



### 2.1基础知识



#### 2.1.1 凸函数

设是定义在实数域上的函数，如果对于任意的实数，都有：
$$
f'' \geq 0
$$
那么是凸函数。若不是单个实数，而是由实数组成的向量，此时，如果函数的 Hessen 矩阵是半正定的，即

$$
H'' \geq 0
$$
是凸函数。特别地，如果$f '' > 0$或者$H'' > 0$ ，称为严格凸函数



#### 2.1.2 Jensen不等式

如下图，如果函数$f$是凸函数，$x$是随机变量，有0.5 的概率是 a，有 0.5 的概率是 b，$x$的期望值就是 a 和 b 的中值了那么：
$$
E[f(x)] = 0.5f(a)+0.5f(b) \geq f(0.5a + 0.5b)= f(E(x))
$$
注：若函数$f$是凹函数，Jensen不等式符号相反。



### 2.2 EM算法推导

对于$n$个相互独立个体$x=(x_1,...,x_n)$，对应隐含数据$z = (z_1,...,z_n)$，此时$(x,z)$为完全数据，样本的模型参数为$\theta$，则观测数据$x_{i}$的概率为$P(x_i|\theta)$，完全数据$(x_i,z_i)$的似然函数$P(x_i,z_i|\theta)$

假如没有隐含变量$z$，我们仅需要找到合适的$\theta$极大化对数似然函数即可
$$
\theta = \arg \max_{\theta} L(\theta)  =\arg \max_\theta  \sum_{i=1}^n \log P(x_i|\theta)
$$
增加隐含变量$z$之后，我们的目标变成了找到合适$\theta$和$z$让对数似然函数极大*：*
$$
\theta , z  = \arg \max_{\theta ,z} L(\theta,z) = \arg\max_{\theta,z} \sum_{i=1}^n \log \sum_{z_i} P(x_i,z_i | \theta)
$$
自然而然会想到分别对未知的$\theta$和$z$分别求偏导。理论是可行的，但由于$ P(x_i | \theta)$是$P(x_i,z_i|\theta)$边缘概率，转化为$ \log P(x_i | \theta)$求导后形式会非常复杂($\log(f_1(x)+f_2(x)+...)$复合函数求导)。那么我们想一下可不可以将加号从 log 中提取出来呢？我们对这个式子进行缩放如下
$$
\sum_{i=1}^n \log \sum_{z_i} P(x_i,z_i|\theta) = \sum_{i=1}^n \log \sum_{z_i} Q_i(z_i) \frac{P(x_i,z_i|\theta)}{Q_i(z_i)} \geq \sum_{i=1}^n \sum_{z_i} Q_i(z_i) \log \frac{P(x_i,z_i|\theta)}{Q_i(z_i)}
$$
上式引入了一个未知的新的分布$Q_i(z_i)$，满足：
$$
\sum_{z} Q_i(z) = 1 , 0 \leq Q_i(z) \leq 1
$$
同时用到了 Jensen 不等式 (对数函数是凹函数)：
$$
\log (E(y)) \geq E(\log (y))
$$
上式实际上是构建了$L(\theta,z)$ 的下界，也就是$\log \frac{P(x_i,z_i|\theta)}{Q_i(z_i)}$的加权求和，由于权值$Q_i(z_i)$累积和为1。也是我们所说的期望，**这就是Expectation的来历**。下一步要做的就是寻找一个合适的$Q_i(z)$最优化这个下界(M步)。

**假设$\theta$已经给定，**那么$\log L(\theta)$的值就取决于$Q_i(z)$和$P(x_i,z_i)$了。我们可以通过调整这两个概率使下界逼近$\log L(\theta)$。由 Jensen 不等式可知，等式成立的条件是随机变量是常数，则有：
$$
\frac{P(x_i,z_i|\theta)}{Q_i(z_i)} = c
$$
其中 c 为常数，对于任意i，我们得到
$$
P(x_i,z_i|\theta) = cQ_i(z_i)
$$
方程两边同时累加和可以得到：
$$
P(x_i|\theta)=\sum_z P(x_i,z_i|\theta) = c \sum_z Q_i(z_i) = c \\
Q_i(z_i)= \frac{P(x_i,z_i|\theta)}{c} = \frac{P(x_i,z_i|\theta)}{P(x_i|\theta)} = P(z_i|x_i,\theta)
$$
从上式可以发现 $Q(z)$是已知样本和模型参数下的**隐变量分布**。

如果$Q_i(z_i)= P(z_i|x_i,\theta)$, 则(7)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要极大化下式：
$$
\arg \max_\theta \sum_{i=1}^n \sum_{z_i} Q_i(z_i) \log \frac{P(x_i,z_i|\theta)}{Q_i(z_i)}
$$
至此，我们推出了在固定参数$\theta$后分布$Q_i(z_i)$的选择问题， 从而建立了$\log L(\theta)$的下界，这是 E 步，接下来的M 步骤就是固定$Q_i(z_i)$后，调整$\theta$，去极大化$\log L(\theta)$的下界

去掉上式中常数的部分$Q_i(z_i)$，则我们需要极大化的对数似然下界为：
$$
\arg \max_\theta \sum_{i=1}^n \sum_{z_i} Q_i(z_i) \log P(x_i,z_i|\theta)
$$
<font color=Red >**Note:**</font> 上式固定了$Q_i(z_i)$去掉了$Q_i(z_i) \log Q_i(z_i)$



### 2.3 EM算法流程

现在我们总结下EM算法的流程。

输入：观察数据$x = (x_1,x_2,...,x_n)$，联合分布$P(x,z|\theta)$，条件分布$P(z|x,\theta)$ ， 极大迭代次数$J$

1. 随机初始化模型参数$\theta$的初值$\theta^0$

2. for i in 1 to J

   - E步：计算联合分布的条件概率期望
     $$
     Q_i(z_i)= P(z_i|x_i,\theta)
     $$

   - M步：极大化$L(\theta)$，得到$\theta$
     $$
     \theta = \arg \max_\theta \sum_{i=1}^n \sum_{z_i} Q_i(z_i) \log P(x_i,z_i|\theta)
     $$

   - 重复E、M步骤直到$\theta$收敛



### 2.4 EM算法另一种理解

坐标上升法（Coordinate ascent）(**类似于梯度下降法，梯度下降法的目的是最小化代价函数，坐标上升法的目的是最大化似然函数；梯度下降每一个循环仅仅更新模型参数就可以了，EM算法每一个循环既需要更新隐含参数和也需要更新模型参数**


<div align=center>
<img src="https://files.mdnice.com/user/1150/01333c22-fe9c-4388-8c7b-58ede05eb3e8.png" width="500" height="" />
</div>

图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。

这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，**E步：**固定 θ，优化Q；**M步：**固定 Q，优化 θ；交替将极值推向极大。





### 2.5 EM算法的收敛性思考

EM算法的流程并不复杂，但是还有两个问题需要我们思考：

1. EM算法能保证收敛吗？

2. EM算法如果收敛，那么能保证收敛到全局极大值吗？　

首先我们来看第一个问题, EM算法的收敛性。要证明 EM 算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大。即：
$$
\sum_{i=1}^n \log P(x_i|\theta^{j+1}) \geq \sum_{i=1}^n \log  P(x_i|\theta^j)
$$
由于
$$
L(\theta,\theta^j) = \sum_{i=1}^n \sum_{z_i} P(z_i|x_i,\theta^j) \log P(x_i,z_i|\theta)
$$
令：
$$
H(\theta,\theta^j) =\sum_{i=1}^n \sum_{z_i} P(z_i|x_i , \theta^j) \log P(z_i|x_i,\theta)
$$
上两式相减得到：
$$
\sum_{i=1}^n \log P(x_i|\theta) = L(\theta,\theta^j) - H(\theta,\theta^j)
$$
在上式中分别取$\theta$为$\theta^j$和$\theta^{j+1}$，并相减得到：
$$
\sum_{i=1}^n \log P(x_i | \theta^{j+1}) - \sum_{i=1}^n \log P(x_i|\theta^j) = [L(\theta^{j+1},\theta^j) - L(\theta^j,\theta^j)] - [H(\theta^{j+1},\theta^j)- H(\theta^j,\theta^j)]
$$
要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。

由于$\theta^{j+1}$使得$L(\theta,\theta^j)$极大，因此有：
$$
L(\theta^{j+1},\theta^j) - L(\theta^j,\theta^j) \geq 0
$$
而对于第二部分，我们有：
$$
H(\theta^{j+1},\theta^j) - H(\theta^j,\theta^j) = \sum_{i=1}^n \sum_{z_i}P(z_i|x_i,\theta^j) \log \frac{P(z_i|x_i,\theta^{j+1})}{P(z_i|x_i,\theta^j)} \\ \leq \sum_{i=1}^n \log(\sum_{z_i} P(z_i|x_i,\theta^j) \frac{P(z_i|x_i,\theta^{j+1})}{P(z_i|x_i,\theta^j)}) = \sum_{i=1}^n \log (\sum_{z_i} P(z_i|x_i,\theta^{j+1}))= 0 
$$
其中第（4）式用到了Jensen不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。

至此，我们得到了: $\sum_{i=1}^n \log P(x_i|\theta_{j+1}) - \sum_{i=1}^n \log P(x_i |\theta^j) \geq 0$，证明了EM算法的收敛性。

从上面的推导可以看出，EM 算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标$L(\theta,\theta^j)$是凸的，则EM算法可以保证收敛到全局极大值，这点和梯度下降法这样的迭代算法相同。

---


[1] https://zhuanlan.zhihu.com/p/36331115

